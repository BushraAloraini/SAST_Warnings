\section{Threats to Validity \label{ttv}}
One possible threat to construct validity is the way that we run static analysis tools and the filtration to only include buffer overflow vulnerabilities. For example, in Flawfinder we included  CWE-20 that indicate ``Improper Input Validation'', which could include other types of vulnerabilities such as SQL injection. However, we further sanitized our hit lists to exclude all unrelated buffer overflow vulnerabilities from all the generated reports. Also,  we manually  analyzed some results to insure that we did not include any non relevant type of bugs. Another threats to construct validity is that our algorithm may not correctly decide a given vulnerability is indeed the same through the checkpoints. We manually analyzed randomly selected 20 vulnerabilities that span in multiple checkpoints in different 10 repositories. We found that it is always the case when the algorithm answers that a given vulnerability is the same through checkpoints, the answer is correct. For example, in some repositories we got a vulnerability line that happened in three different checkpoints . The three line number could have a difference of 300 lines, however the algorithm successfully determined that the line of the code is the same across the checkpoints. We found one  case that the algorithm rejected the matching and that was also correct answer. However, further investigation might be needed to include more cases to be manually  analyzed. 

One possible threat to internal validity could be the chosen number and type  repertoires. As the findings might be related to the very large size if repertoires, we might get different conclusion with small onces. In fact, even though we intended to study large program,  our dataset included  smaller once as well as it was shown in Table \ref{stat}. Also, as we analyzed only 40 repertoires we could not obtain  robust results. Therefore, we will improve our dataset in the future in terms of varying the repositories types and the size. Thus, in the future we will include 400 repositories that vary in size. Also, our analysis regrading false and true positives was mainly the removal of the code of line. However, that conclusion  might not be always true as the removed bug could be due to a change on the project design. Therefore, in the future we could apply more advanced techniques such as program slicing to get more solid conclusions. One threats to external validity is that we could not generalize our result to all other static analysis tools. However, we believe that our current algorithm could fit more advanced  static analysis tools.  In the future, we aim to include more advanced tools. 
 