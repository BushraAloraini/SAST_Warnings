

We will conduct the study by examining the following RQs: 

\begin{itemize}
\item \textbf{RQ1: How are vulnerabilities distributed across types?}
In this research question, we aim at discovering
the number of different types of vulnerabilities detected by SATs
and removed from the source code. We will be tracing the lines of codes
during repository history to gain the line of codes changes. 
Similarly to ~\cite{di2009life},  we will apply a proportion test
that helps showing whether the proportion of removed vulnerabilities 
differ across vulnerability types.  
This test could help us to highlight any vulnerability types
which is discovered and removed more than the other. 
Also, for each class of vulnerability, 
we will calculate the odds ratio (OR) 
which shows the likelihood of an event will occur versus it not occurring. 
OR is the ratio of the odds of an event occurring
in one set (in this research the removed vulnerabilities subset)
to the odds of it occurring in another set (in this research the alive vulnerabilities subset). 


One of the challenges that we might face is that
the vulnerability classification used by the SATs is different in each of the tool, 
making difficult to conduct the analysis. 
Therefore, we will be using the the seven pernicious kingdoms classification ~\cite{tsipenyuk2005seven} 
to classify the  warnings generated by SATs. 
We have lists of vulnerabilities that each tool claims to detect. 
So, we will first classify these vulnerability types to the seven
pernicious kingdoms classification and based on the tool output
to consider the subtypes of the seven pernicious kingdoms classification.
Then, we could measure the distribution of different categories for each tool.

\item \textbf{RQ2: How do the vulnerabilities evolve over time?}
We focus here on vulnerability age which is the time interval
between the introduction and removal of a vulnerability.  
In particular, we want to measure how long it takes to fix a vulnerability
and whether a specific type of vulnerabilities has an influence on that. 
In this research question, we will focus on true positive vulnerabilities
that were detected correctly by SATs, and removed from the code. 
Whenever we find a vulnerability that was removed, 
we trace the line of code that contains the vulnerability backward
during the history to find the introduction time.  
We could generate snapshot each six months, to minimize the running time.
If a vulnerability introduction were detected we could flag that version
to be `` vulnerability introduction '' version. 
After that we could trace the line of code forward to discover
when that vulnerability was removed and flag that to be `` vulnerability removal '' version. 
So, after having the vulnerability age, 
we could then measure the ages related to each vulnerability type.  
Also similarly to ~\cite{di2009life}, this research question studies
whether the age of different vulnerability types follow a specific probability distribution.



\item \textbf{RQ3: How are vulnerabilities removed?}

In this research question, we distinguish cases where the vulnerability was:
(1) removed because of line removal, 
(2) removed because of line change in the same line, 
(3) removed, but lines contain the vulnerabilities are unmodified 
but there is changes in other lines, 
or (4) removed and documented in the commit note.

Similarly to \cite{di2009life}, 
we will use a proportion test and Odds Ratio(OR) 
to check whether there is a significant difference
across vulnerability types for the above cases. 


\item \textbf{RQ4: How are vulnerabilities detected 
by the project team distributed across different categories?}

In this research question,
we goal to extract the pattern of the fixed vulnerabilities
that were detected and fixed by the project team. 
The main idea here is to mine the repository history
and extract some facts about real vulnerabilities
that belong to specific vulnerability categories.
We will be using the seven pernicious kingdoms classification \cite{tsipenyuk2005seven}
as it is shown in Table ~\ref{Seven}.  
Mainly, we will be utilizing a heuristic for classifying the commit messages
similar to that one followed in ~\cite{mockus2000identifying}.
For each project, we will be mining the commit messages
searching for vulnerability-related keywords,
such as  error,  vulnerability,  fix,  issue, mistake, incorrect,
fault, defect, crash, and flaw in commit messages. 
Then, we will be further mining those messages to look for vulnerabilities
that have clear message about the related problem, 
and based on that we will assign them to the seven pernicious kingdoms classification ~\cite{tsipenyuk2005seven}. 
For instance, if we get the commit message `` - fix a memory leak'',
this will be classified to ``Indicator of Poor Code Quality''.  

\begin{table}[ht]
\centering
\scriptsize
\caption{Seven Pernicious Kingdoms.}
\label{Seven}
\begin{tabular}{||p{.5cm}|p{6cm} ||}
\hline
\textbf{\#} & \textbf{Seven Pernicious Kingdoms
} \\
\hline\hline
1& Input Validation and Representation  \\
2& Improper Fulfillment of API Contract ('API Abuse')  \\
3& Security Features  \\
4& Time and State  \\
5& Error Handling  \\
6& Indicator of Poor Code Quality  \\
7& Insufficient Encapsulation  \\
* & Environment  \\
\hline
\end{tabular}
\end{table}


\item \textbf{RQ5: How effective the studied static analysis tools are?}
In this study, we want to study the capabilities and the effectiveness
of static analysis tools in C++. 
Knowing some real vulnerabilities found by the project team from RQ4,
we will measure the effective of the studied static analysis tools in terms of
precision, recall, F-measures, and running time. 
In this research question, we will be running six static analysis tools (SATs)
shown in Table ~\ref{SATs} on two versions of each repository
with 5 years gap between the tow versions. 


In our study, we define \textbf{false positives} as the number of vulnerabilities 
that are flagged by SATs but, in fact, they are not. 
While \textbf{true positives} is the number of vulnerabilities that
are flagged by SATs and, in fact, they are. 
\textbf{false negative} are vulnerabilities that have been detected
by other but not by the SATs. 
We will be extracting false positive and true positives by observing
how vulnerability reports and line of code evolve over time. 
vulnerability reports that still reappear in later versions may indicate false positive reports. 
While vulnerability reports that disappear in later versions could indicate true positive,
however this set of vulnerability reports will be further studied
to insure that the vulnerability disappear due to a vulnerability fix.  
In this study, in case of that a vulnerability report disappeared, 
we will not consider it to be true positive if it follows any of the following cases:

\begin{itemize}
\item file that includes a vulnerability line was deleted.
\item a version that indicts a removal of a feature.
\item a version that indicts code redesigning or refactoring.
\end{itemize}


Then, we will be investigating the true positives to see weather those vulnerabilities
match the real vulnerabilities set that we have from RQ4. 
When measuring the recall which depends on the false negative,
we will treat each tool uniquely. 
Meaning that we will only match the subset of issues that a tool claims to detect. 

\begin{table}[ht]
\centering
\scriptsize
\caption{Static Analysis Tools for C/C++}
\label{SATs}
\begin{tabular}{||p{.5cm}|p{6cm} ||}
\hline

 \textbf{\#} & \textbf{Static Analysis Tools for C/C++
} \\
\hline
\hline
 1 & RATS \\
2 & Flawfinder  \\
3 & Cppcheck  \\
4 & Clang Static Analyzer  \\
5 & Parasoft C/C++test \\
6 & PVS-studio  \\
\hline
\end{tabular}
\end{table}
 \end{itemize}
